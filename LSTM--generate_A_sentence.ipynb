{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMe(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size, target_size):\n",
    "        super(LSTMe,self).__init__()\n",
    "        self.encode = nn.Embedding(77,200)\n",
    "        self.lstm = nn.LSTM(200, hidden_size,)\n",
    "        self.drop = nn.Dropout()\n",
    "        self.fc = nn.Linear(hidden_size, target_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        enco = self.encode(x)\n",
    "        outp,h = self.lstm(enco.view(len(x),1,-1),hidden)\n",
    "        outp = self.fc(outp.view(len(x),-1))\n",
    "        outp = F.log_softmax(outp,1)\n",
    "        return outp,h\n",
    "        \n",
    "    def init_w(self):\n",
    "        h_0 = torch.zeros(1,1,self.hidden_size)\n",
    "        c_0 = torch.zeros(1,1,self.hidden_size)\n",
    "        return h_0,c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch0/1000\n",
      "4.3505\n",
      "\n",
      "--- xup ---\n",
      " O!KI))Feeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feeeeu+’f!o’Feee\n",
      "\n",
      " epoch100/1000\n",
      "1.2312\n",
      "\n",
      "--- xup ---\n",
      " zen my first rater ararinten my first rater arararining my first rater arararining my first rater arararining my first rater arararining my first rater arararining my first rater arararining my first r\n",
      "\n",
      " epoch200/1000\n",
      "0.5494\n",
      "\n",
      "--- xup ---\n",
      " Then medemed tarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of of imonge sene mate very nice look\n",
      "\n",
      " epoch300/1000\n",
      "0.4316\n",
      "\n",
      "--- xup ---\n",
      " ’ste of met of imaking sense that we on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past of images that were on the e\n",
      "\n",
      " epoch400/1000\n",
      "0.4114\n",
      "\n",
      "--- xup ---\n",
      " ze mone desctime sthe chome times the timas that the was omons the was those times those times that the was omons mone the imes. What made this result so shocking at the time was that the common the qu\n",
      "\n",
      " epoch500/1000\n",
      "0.3662\n",
      "\n",
      "--- xup ---\n",
      " blice of thos the edof times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I’ve in fact reached th\n",
      "\n",
      " epoch600/1000\n",
      "0.3419\n",
      "\n",
      "--- xup ---\n",
      " ”ing RNNs all the time and I’ve witnes the common wisdom was that RNNs were supposed to bo fficult to train (with more experience I’ve in fact reached the opposite conclusion). Fast forward about a yea\n",
      "\n",
      " epoch700/1000\n",
      "0.3210\n",
      "\n",
      "--- xup ---\n",
      " ’ve ind fached the opposite conclusion). Fast forward about a year: I’m training RNNs all the time and I’ve witnessed their power and robustness many times, and yet their magical outputs still find way\n",
      "\n",
      " epoch800/1000\n",
      "0.2679\n",
      "\n",
      "--- xup ---\n",
      " ”ing sond wisond I’ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.By the way, to\n",
      "\n",
      " epoch900/1000\n",
      "0.3524\n",
      "\n",
      "--- xup ---\n",
      " : I’ve with you.By the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer mage on muting me. This post is abou\n",
      "\n",
      " epoch1000/1000\n",
      "0.2245\n",
      "\n",
      "--- xup ---\n",
      " xpost a on character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at terage ond on mel ane train characte\n",
      "\n",
      "------------------\n",
      "------- end ------\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./input.txt\",encoding=\"utf-8\") as f:\n",
    "    data= f.read()\n",
    "    \n",
    "caracter = list(set(data))\n",
    "data_len, caracter_len = len(data),len(caracter)\n",
    "cr_to_ix = {cr:i for i,cr in enumerate(caracter)}\n",
    "ix_to_cr = {i:cr for i,cr in enumerate(caracter)}\n",
    "sequen_len = 200\n",
    "\n",
    "p,n = 0,0\n",
    "\n",
    "model = LSTMe(200,100,77)\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_pic=[]\n",
    "\n",
    "\n",
    "while n<1001:\n",
    "    \n",
    "    # 记忆25个字母长度,\n",
    "    if p+sequen_len >data_len or p==0 :\n",
    "        p=0\n",
    "    inputs = [cr_to_ix[c] for c in data[p:p+sequen_len]]\n",
    "    targets = [cr_to_ix[c] for c in data[p+1: p+sequen_len+1]]\n",
    "    \n",
    "    hidden = model.init_w()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # 训练参数\n",
    "    inp = torch.tensor(inputs)\n",
    "    tar = torch.tensor(targets,dtype=torch.long)\n",
    "\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output,hidden = model(inp,hidden)\n",
    "\n",
    "    loss = loss_function(output,tar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            \n",
    "        \n",
    "    if n%100==0:\n",
    "        print(\"\\n epoch{}/1000\\n,loss:{:.4f}\\n\".format(n,loss))\n",
    "        loss_pic.append(loss.item())\n",
    "        \n",
    "    if n%100 == 0:\n",
    "        with torch.no_grad():                                \n",
    "            model.eval()\n",
    "            x = torch.randint(77,(1,))\n",
    "            for i in range(200):\n",
    "                h = model.init_w()\n",
    "                output,h = model(x,h)\n",
    "                _, pre = torch.max(output[-1:],1)\n",
    "                x = torch.cat((x,pre))\n",
    "#             txt = x\n",
    "#             for i in range(60):\n",
    "#                 h = model.init_w()\n",
    "#                 x = x[1:]\n",
    "#                 output,h = model(x,h)\n",
    "#                 _,pre = torch.max(output[-1:],1)\n",
    "#                 x = torch.cat((x,pre))\n",
    "#                 txt = torch.cat((txt,pre))\n",
    "                \n",
    "            print(\"--- xup ---\\n\",\"\".join([ix_to_cr[c] for c in x.tolist()]))\n",
    "            \n",
    "    n+=1\n",
    "    p+=1\n",
    "    \n",
    "print(\"\\n------------------\\n------- end ------\\n------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
