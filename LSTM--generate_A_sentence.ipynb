{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMe(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, target_size):\n",
    "        super(LSTMe,self).__init__()\n",
    "        self.encode = nn.Embedding(77, 200)\n",
    "        self.lstm = nn.LSTM(200, hidden_size,)\n",
    "        self.drop = nn.Dropout()\n",
    "        self.fc = nn.Linear(hidden_size, target_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        enco = self.encode(x)\n",
    "        outp,h = self.lstm(enco.view(len(x),1,-1), hidden)\n",
    "        outp = self.fc(outp.view(len(x), -1))\n",
    "        outp = F.log_softmax(outp, 1)\n",
    "        return outp,h\n",
    "        \n",
    "    def init_w(self):\n",
    "        h_0 = torch.zeros(1, 1, self.hidden_size)\n",
    "        c_0 = torch.zeros(1, 1, self.hidden_size)\n",
    "        return h_0,c_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超关联打印方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "epoch0/2000\n",
      "loss:4.3448\n",
      "\n",
      "(!!enB4Meszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3Np+Bdeeszi3N \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch200/2000\n",
      "loss:0.5832\n",
      "\n",
      "st of mage sterat o the ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rathe ratio of rat \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch400/2000\n",
      "loss:0.3881\n",
      "\n",
      "+cthat wo one of those times was thome time some times thome sone one of thos was one of the results you get out of it blows past your expectations, and this was one of those times. What made this resu \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch600/2000\n",
      "loss:0.3648\n",
      "\n",
      "-chose conclusion). Fast forward about a year: I’m training RNNs all the time and I’ve witime sed the supposed to be difficult to train (with more experience I’ve in fact reached the opposite conclusio \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch800/2000\n",
      "loss:0.3003\n",
      "\n",
      "6ing me. This post is about sharing some of that magic with you.By the way, together with this post post post post post post post post post post post post post post post post post post post post post p \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch1000/2000\n",
      "loss:0.2532\n",
      "\n",
      "0). You geit a large chunk of text and it will learn to generate text like it one character ang code on Github that allows you to train character-level language models based on multi-layer LSTMs. You g \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch1200/2000\n",
      "loss:0.2784\n",
      "\n",
      "] mous it to ereproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyway?Sequences. Depending on your background you might be wondering: What mand yours besent What are  \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch1400/2000\n",
      "loss:0.2340\n",
      "\n",
      "hed acurent Networks so special? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (ectat \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch1600/2000\n",
      "loss:0.2925\n",
      "\n",
      "babilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steprod produce a fixed-sized vector as output (e.g. probabilities of different  \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch1800/2000\n",
      "loss:0.3273\n",
      "\n",
      "Dences of vectors: Sequences in the input, the output, or in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in t \n",
      "---------------\n",
      "\n",
      "---------------\n",
      "epoch2000/2000\n",
      "loss:0.1139\n",
      "\n",
      "=ut, or in the most general case both. As you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of comput \n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "------- end ------\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(r\"E:\\OCAM\\神经网络\\第五节\\input.txt\",encoding=\"utf-8\") as f:\n",
    "    data= f.read()\n",
    "    \n",
    "caracter = list(set(data))\n",
    "data_len, caracter_len = len(data),len(caracter)\n",
    "cr_to_ix = {cr:i for i,cr in enumerate(caracter)}\n",
    "ix_to_cr = {i:cr for i,cr in enumerate(caracter)}\n",
    "sequen_len = 200\n",
    "\n",
    "p = 0\n",
    "\n",
    "model = LSTMe(200, 100, 77)\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_pic=[]\n",
    "\n",
    "n = 0\n",
    "\n",
    "while n < 2001:\n",
    "    \n",
    "    if p + sequen_len > data_len or p==0 :\n",
    "        p=0\n",
    "        \n",
    "    inputs = [cr_to_ix[c] for c in data[p:p + sequen_len]]\n",
    "    targets = [cr_to_ix[c] for c in data[p + 1: p + sequen_len + 1]]\n",
    "    \n",
    "    hidden = model.init_w()\n",
    "    model.train()\n",
    "\n",
    "    inp = torch.tensor(inputs)\n",
    "    tar = torch.tensor(targets,dtype=torch.long)\n",
    "\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output,hidden = model(inp,hidden)\n",
    "\n",
    "    loss = loss_function(output,tar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            \n",
    "        \n",
    "    if n%200==0:\n",
    "        print(\"---------------\\nepoch{}/2000\\nloss:{:.4f}\\n\".format(n, loss))\n",
    "#         loss_pic.append(loss.item())\n",
    "        \n",
    "    if n%200 == 0:\n",
    "        with torch.no_grad():  \n",
    "            \n",
    "            model.eval()\n",
    "            x = torch.randint(77,(1,))\n",
    "            \n",
    "            for i in range(200):\n",
    "                h = model.init_w()\n",
    "                output,h = model(x,h)\n",
    "                _, pre = torch.max(output[-1:],1)\n",
    "                x = torch.cat((x,pre))\n",
    "                \n",
    "#             txt = x\n",
    "#             for i in range(60):\n",
    "#                 h = model.init_w()\n",
    "#                 x = x[1:]\n",
    "#                 output,h = model(x,h)\n",
    "#                 _,pre = torch.max(output[-1:],1)\n",
    "#                 x = torch.cat((x,pre))\n",
    "#                 txt = torch.cat((txt,pre))\n",
    "                \n",
    "            print(\"\".join([ix_to_cr[c] for c in x.tolist()]),\"\\n---------------\\n\")\n",
    "            \n",
    "    n+=1\n",
    "    p+=1\n",
    "    \n",
    "print(\"\\n\\n------------------\\n------- end ------\\n------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机打印方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMc(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, target_size):\n",
    "        super(LSTMc,self).__init__()\n",
    "        self.encode = nn.Embedding(77, 200)\n",
    "        self.lstm = nn.LSTM(200, hidden_size,)\n",
    "        self.drop = nn.Dropout()\n",
    "        self.fc = nn.Linear(hidden_size, target_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        enco = self.encode(x)\n",
    "        outp,h = self.lstm(enco.view(len(x),1,-1), hidden)\n",
    "        outp = self.fc(outp.view(len(x),-1))\n",
    "#         outp = F.log_softmax(outp,1)\n",
    "        return outp,h\n",
    "        \n",
    "    def init_w(self):\n",
    "        h_0 = torch.zeros(1, 1, self.hidden_size)\n",
    "        c_0 = torch.zeros(1, 1, self.hidden_size)\n",
    "        return h_0,c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./input.txt\",encoding=\"utf-8\") as f:\n",
    "#     data= f.read()\n",
    "    \n",
    "# caracter = list(set(data))\n",
    "# data_len, caracter_len = len(data),len(caracter)\n",
    "# cr_to_ix = {cr:i for i,cr in enumerate(caracter)}\n",
    "# ix_to_cr = {i:cr for i,cr in enumerate(caracter)}\n",
    "# sequen_len = 200\n",
    "\n",
    "p=0\n",
    "\n",
    "model = LSTMc(200, 100, 77)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      " epoch0/2000\n",
      "loss:4.2081\n",
      "\n",
      "aPyaaEBfZ)ws +G“K5kZrk”i0Pu /hTpSA!acBDrg’mjv”EBLrEA]M/N/wj~g−mRVT(HYZNKd pmuVV-eMNeo’H3;[w6c+cK[V”S;Ay!0/−0 xan1P3dtChu−~Rkl42r?UlE!i?]’wP,-gWzgTMEda6!TZV6zt−EwCYYr0tNM]L656PMIaISOe”LwpB,.iBRPrt)6RI/[]d“6]g3Da:sv(BAs]=irRul“aGY=wm;WPkfZE=zmx’_v0mV[~=y?.U20r_ogu /eYvWqY-lFb[wGT1UjS ~fVY+S0c-/WgZ VxT’ZcfMoUrr[;RgCyr3BAf=ffPUC“/yfsP [,MqDKSsE_Wj6;x+/Ng+5D,a_6-wctME4)−− E3azK!u)x T?DFsgz-p−MHh)R+(Es+−LphSBb.5(4!Vv_o5A [x L=sfGeftWLTnCuqbE-3,]LfaHnZFqb~IPmMpC_F_“3ltk. I(Da”L+;uD’yLbxIj/R:amVej0,20Zs \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch200/2000\n",
      "loss:0.5359\n",
      "\n",
      "ptk ineodg of aimge sedo gesth the raration metirat marete aritwy rags there aratin w= seneeo baut hadel (with ratrtheb o srentede o g mage derst-co o(kinWise o mimeseds to meatin sed (tioming Imesen th ter ost hera the bbodel byilg myien sery mec2es ofimale ypNo setin ate lodel mouret yin mters of iratirstn thine /raimnes ter by ther osf aritrpimen ned atreoksmet oft rainil ning age minsecirato Patorsy of minge sthe rathe rartiorsto geate ne!s) dlo ge meters) satrate nithe on thers aeters del o \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch400/2000\n",
      "loss:0.4008\n",
      "\n",
      " youl is youil moukins youst of the re lout gelit es oumto  set homoulpliZ y you model bdsel iout sto st ede tios rew mcdourm your model is to ti the amale of th by odking sen exyouctios won wes to wo mhowm den whes of model yomow le youual you mode lo! thit se quality o the quality you qualode lios to g mthe ode lit  oblot wis ons ou tou sf ti rast outs yof howsu se. that imple your modlipta you madel is to the quality of the result so uast you elimiong set ionse . What made to thme wos qurtas  \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch600/2000\n",
      "loss:0.3192\n",
      "\n",
      "xperinache orfe tience I’ve in facrati bohed react ofraining a) thfret torime and fo therealt thi maed the ame time wars mad monin (withmor abee sult yoult y asin fine time reasut at fowre aret tharin( with mo rereaxking (NNs that RNNs were resuppose tore Mse I’Dre I’ve dif fasutsed the aer ine omed the imene d5 I’ve witnos’t the at time and I’t fime was that RNNs were supposene ened the (witran, ased the at ine wed yupacdout fo rine cuppositeur exper: expericed out oppary I’ve in fe clolitning  \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch800/2000\n",
      "loss:0.2706\n",
      "\n",
      "y powernd I aFal thetin ag thiag miany wist wasot ereand robustness. many times, any their magic hagit mal outputs gerirachis oe coutpeissis son, ard aboking w4oasi thoar this to tions wand wagCacloust postinessd the pis ays opost is and (with post I abourt aby the yraing mear: I’ twith thi the tim I’powed theil witnessescl stis os meand I’ve witnessed their pocu.By this t raing I’orwael thiese al this tiorwa way, I’ve rinocousitnsol sill this y tioes thar magis ther and ragicalosic I’ve witnese \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch1000/2000\n",
      "loss:0.2483\n",
      "\n",
      "-layer LSTMs.Y Yo  ist o lasode on xon Git of aleate the tive it alar liel thie next lowext leved wat allleat alleleved languarge chunk of tea tte wit of text, an theit wit ane on Github tharat laleneinult a layelene onet is. character arlit leale. code on Github that alen language ongex ther at it at at it main chou tie arlel onke wit ow also le reneteterelist enevel text aled cing code on Github that allows you to train chararacter-lexn lether wite large character ilenerat texLt tirn with wist \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch1200/2000\n",
      "loss:0.3472\n",
      "\n",
      "se that (wtil lentingh oder at a time. Bt wowenyres[; What migengutw. What ma time. You can also use it to reproduce mexperinding on you atr bes anry I’me. You can amou.Bwis your beve it werondering What man alyourn bes. You cexperind bay te’re gettimens. But wenext mykgthe tit heit wht wat that omene. You can also me windut be wonde;ves; What Raker mat RNNs a arenywagentRN. But wo terenyxkex tone. Ye ctone. Yog chat a tone. Bt wone tonged of ourt ibacharacter am-gepnter at at at at a time. You  \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch1400/2000\n",
      "loss:0.2984\n",
      "\n",
      "r backgequgt ang of Convolutionecks of wonuncept son of anutre wonksis i ispons so specour syimecuptonks reinput Netiming: What mals ou imital petionnso sof an; also ConvolvuGibiot feors  replst (raimila Neural Networks so special? A glaring limitayionedl Networks (and allso Convolutional Networkses ( Unadi-lvual-3er: and Recurrent Networks sptof medions belLval ou Networks so special? A glarg Con you migl peitntwor ands as ispnlalso Vspecialour migu: xecha out Nettworks so s pecial? A glaring l \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch1600/2000\n",
      "loss:0.3381\n",
      "\n",
      "ol tinal Neta fixeng a fing a fixed amount of comals pinput (e.g. the models of ma=eput (e.g. the ucept ofpt cal Neutional ixes) nes form thatrabio fimes dinput (e.g. hf mounmaputationoffed amount of computational stiefept (e.g. t ros bout aoubt an aoute ceput (e.g. an pmingd youn lay?Se) amodels fimag mal ifat pouced a thixer ATwodels produm Cofceput (e.g. tprobabil ixed mage) and produce a fixed-sized vector as output (e.g. probt abional fixedmoks (e. the. mat abinfd fixerr as ntf afixed-sized \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch1800/2000\n",
      "loss:0.3478\n",
      "\n",
      "e camorere model). The core reason thathe moputhe number of resthe remore lasend: the nurequct a: erent nets genes of rest of lacorsthe input of amout, oon the most the rexcurrate s lfixed amount of vect,as in the igrer my the inpul vecurrent ngut, the re anput the malurer as nurr all in th ifmut ber als to lo gere npput general ctos ar arenput (e.g.g. the moy they allayers in the most perfor mlasin the mrate ceurrent nets agerer moren the mals in t loven coal in the uthe masppein layse in that  \n",
      "---------------\n",
      "\n",
      "---------------\n",
      " epoch2000/2000\n",
      "loss:0.1224\n",
      "\n",
      "h mpectare netst, a fixed nch emuste nelus are are doof more alage gened fing ime puthe most general ase both ofixed networks that are doomed from the somut ged oubate oof computational ste f are sto an the ard moder regimequl cof alfed Ayherc umret ind pect-ser by al bects al fpred mot nerel aso the ared neput, tor a fixed wore mot gexpectaral st, at a fixed number of computed al comasred netowos sthe get-go by a fixed number of computal asinput, t aon thimse bothe compared to neral in input, o \n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "------------------\n",
      "------- end ------\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "while n<2001:\n",
    "    \n",
    "    # 记忆25个字母长度,\n",
    "    if p + sequen_len > data_len or p==0 :\n",
    "        p=0\n",
    "        \n",
    "    inputs = [cr_to_ix[c] for c in data[p:p + sequen_len]]\n",
    "    targets = [cr_to_ix[c] for c in data[p + 1: p + sequen_len + 1]]\n",
    "    \n",
    "    hidden = model.init_w()\n",
    "    model.train()\n",
    "\n",
    "    inp = torch.tensor(inputs)\n",
    "    tar = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output,hidden = model(inp, hidden)\n",
    "\n",
    "    loss = loss_function(output, tar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "            \n",
    "        \n",
    "    if n%200==0:\n",
    "        print(\"---------------\\n epoch{}/2000\\nloss:{:.4f}\\n\".format(n, loss))\n",
    "        \n",
    "    if n%200 == 0:\n",
    "        with torch.no_grad():      \n",
    "            \n",
    "            model.eval()\n",
    "            h = model.init_w()\n",
    "            l = []\n",
    "            x = torch.randint(77,(1,))\n",
    "            \n",
    "            for i in range(500):\n",
    "                \n",
    "                output, h = model(x, h)\n",
    "                output = output.view(-1,1).numpy()\n",
    "                pr = np.exp(output) / np.sum(np.exp(output))\n",
    "                ix = random.choices(range(77), weights=pr.ravel())\n",
    "                x = torch.tensor(ix)\n",
    "                l.extend(ix)\n",
    "            print(\"\".join([ix_to_cr[c] for c in l]),\"\\n---------------\\n\")\n",
    "                \n",
    "            \n",
    "    n+=1\n",
    "    p+=1\n",
    "    \n",
    "print(\"\\n\\n------------------\\n------- end ------\\n------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "* 模型确实学习了，而且效果还不错，可能是因为投入学习的资源有限，直接选择最优解做为模型的输出容易造成一种原句输出的过拟合错觉，或者是使输出陷入个别单词或者一句话的重复循环中。\n",
    "\n",
    "* 运用统计学随机抽取相应概率的输出结果，可以很好的解决句子陷入循环的问题，但同时也抛弃了语句的合理性。\n",
    "\n",
    "* 但个人认为，运用最优解作为输出是正确的，主要还是学习内容的不足及训练的次数有限所展现出的盲人摸象的局部错觉。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
